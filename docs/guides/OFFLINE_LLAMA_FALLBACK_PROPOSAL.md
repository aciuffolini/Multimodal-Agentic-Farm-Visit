# ü¶ô Offline Llama Fallback Proposal

## üéØ Problem Statement

**Current Situation:**
- Primary: Gemini Nano (Android 14+ with AICore) ‚úÖ
- Issue: Many devices don't have Android 14+ or AICore support ‚ùå
- Need: **Second offline option** for incompatible devices

**Requirement:**
- ‚úÖ Fully offline (no internet)
- ‚úÖ Text-only (no vision needed for fallback)
- ‚úÖ Works on Android 7+ (wider compatibility)
- ‚úÖ Lightweight model (< 500MB)
- ‚úÖ Reasonable performance on mid-range devices

---

## üí° Proposed Solution: 3-Tier Offline Architecture

```
Priority Order:
1. Gemini Nano (if available) ‚Üí Android 14+, AICore
2. Llama Local (fallback) ‚Üí Any Android 7+
3. Cloud API (optional) ‚Üí Only if online and user provides key
```

### Architecture Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         ChatDrawer.tsx                           ‚îÇ
‚îÇ  try {                                           ‚îÇ
‚îÇ    geminiNano.stream() ‚Üí Priority 1            ‚îÇ
‚îÇ  } catch {                                       ‚îÇ
‚îÇ    try {                                         ‚îÇ
‚îÇ      llamaLocal.stream() ‚Üí Priority 2           ‚îÇ
‚îÇ    } catch {                                     ‚îÇ
‚îÇ      cloudAPI.stream() ‚Üí Priority 3 (optional)  ‚îÇ
‚îÇ    }                                             ‚îÇ
‚îÇ  }                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîß Technical Implementation

### Option 1: ONNX Runtime Mobile (Recommended)

**Why ONNX Runtime:**
- ‚úÖ Cross-platform (works on Android, iOS, Web)
- ‚úÖ Hardware acceleration (GPU, NPU if available)
- ‚úÖ Optimized for mobile inference
- ‚úÖ Supports quantized models (INT8, INT4)
- ‚úÖ Active maintenance by Microsoft

**Model Choice:**
- **Llama 3.2 3B** (quantized INT8) ‚Üí ~2.5GB ‚Üí Compress to ~500MB
- Or **TinyLlama 1.1B** (quantized) ‚Üí ~400MB (faster, less quality)

**Implementation:**
1. Convert Llama to ONNX format
2. Quantize to INT8 (4x size reduction)
3. Bundle in APK or download on first use
4. Use ONNX Runtime Mobile Java API

### Option 2: TensorFlow Lite (Alternative)

**Why TFLite:**
- ‚úÖ Google's official mobile ML framework
- ‚úÖ Already used by Gemini Nano (familiar)
- ‚úÖ Excellent Android integration
- ‚úÖ Supports quantization

**Model Choice:**
- TensorFlow Lite version of Llama 2/3
- Convert from Hugging Face format

---

## üì± Android Implementation Plan

### 1. Create Capacitor Plugin: `LlamaLocalPlugin.java`

```java
package com.farmvisit.app;

import com.getcapacitor.Plugin;
import com.getcapacitor.PluginCall;
import com.getcapacitor.PluginMethod;
import com.getcapacitor.annotation.CapacitorPlugin;

// ONNX Runtime imports
import ai.onnxruntime.*;

@CapacitorPlugin(name = "LlamaLocal")
public class LlamaLocalPlugin extends Plugin {
    
    private OrtEnvironment env;
    private OrtSession session;
    private boolean modelLoaded = false;
    
    @PluginMethod
    public void isAvailable(PluginCall call) {
        JSObject ret = new JSObject();
        
        // Check if model file exists
        boolean modelExists = checkModelFile();
        ret.put("available", modelExists);
        ret.put("modelSize", getModelSizeMB());
        
        call.resolve(ret);
    }
    
    @PluginMethod
    public void loadModel(PluginCall call) {
        // Load ONNX model from assets or downloaded file
        // Initialize ONNX Runtime
        // Model will be ~500MB - can download on first use
    }
    
    @PluginMethod
    public void generate(PluginCall call) {
        String prompt = call.getString("prompt", "");
        // Run inference using ONNX Runtime
        // Return generated text
    }
    
    @PluginMethod
    public void stream(PluginCall call) {
        // Similar to GeminiNanoPlugin.stream()
        // Use Handler.postDelayed for non-blocking chunks
    }
}
```

### 2. TypeScript Wrapper: `LlamaLocal.ts`

```typescript
// apps/web/src/lib/llm/LlamaLocal.ts

import { Capacitor } from '@capacitor/core';
import LlamaLocalNative from './LlamaLocalNative';

export class LlamaLocal {
  private initialized: boolean = false;
  private available: boolean = false;

  async checkAvailability(): Promise<boolean> {
    if (!Capacitor.isNativePlatform()) {
      // Web: not available
      return false;
    }

    const result = await LlamaLocalNative.isAvailable();
    this.available = result.available;
    return this.available;
  }

  async initialize(): Promise<void> {
    if (!Capacitor.isNativePlatform()) {
      throw new Error('Llama Local only available on Android');
    }

    await LlamaLocalNative.loadModel();
    this.initialized = true;
  }

  async *stream(input: { text: string }): AsyncGenerator<string> {
    if (!this.initialized) {
      await this.initialize();
    }

    // Similar streaming logic to GeminiNano
    const generator = LlamaLocalNative.stream({ prompt: input.text });
    for await (const chunk of generator) {
      yield chunk;
    }
  }
}

export const llamaLocal = new LlamaLocal();
```

### 3. Unified LLM Provider: `LLMProvider.ts`

```typescript
// apps/web/src/lib/llm/LLMProvider.ts

import { geminiNano } from './GeminiNano';
import { llamaLocal } from './LlamaLocal';
import { streamChat } from '../api'; // Cloud fallback

export class LLMProvider {
  async *stream(input: { text: string; location?: { lat: number; lon: number } }): AsyncGenerator<string> {
    // Priority 1: Try Gemini Nano
    try {
      const available = await geminiNano.isAvailable();
      if (available) {
        console.log('[LLMProvider] Using Gemini Nano');
        yield* geminiNano.stream(input);
        return;
      }
    } catch (err) {
      console.warn('[LLMProvider] Gemini Nano failed:', err);
    }

    // Priority 2: Try Llama Local
    try {
      const available = await llamaLocal.checkAvailability();
      if (available) {
        console.log('[LLMProvider] Using Llama Local');
        yield* llamaLocal.stream(input);
        return;
      }
    } catch (err) {
      console.warn('[LLMProvider] Llama Local failed:', err);
    }

    // Priority 3: Cloud API (if online and configured)
    if (navigator.onLine) {
      try {
        console.log('[LLMProvider] Using Cloud API');
        const messages = [{ role: 'user', content: input.text }];
        yield* streamChat(messages, { visit: { gps: input.location } });
        return;
      } catch (err) {
        console.warn('[LLMProvider] Cloud API failed:', err);
      }
    }

    throw new Error('No LLM provider available');
  }
}

export const llmProvider = new LLMProvider();
```

### 4. Update ChatDrawer

```typescript
// In ChatDrawer.tsx - replace geminiNano.stream() with:

import { llmProvider } from '../lib/llm/LLMProvider';

// In send() method:
const generator = llmProvider.stream({
  text: userInput,
  location: visitContext?.gps 
    ? { lat: visitContext.gps.lat, lon: visitContext.gps.lon }
    : undefined,
});
```

---

## üì¶ Model Management

### Option A: Bundle in APK (Larger APK, but works immediately)
- APK size: ~50MB (base) + ~500MB (model) = **~550MB total**
- ‚úÖ Works offline immediately
- ‚ùå Large download

### Option B: Download on First Use (Recommended)
- APK size: ~50MB (base)
- Model download: ~500MB (one-time, cached)
- ‚úÖ Smaller initial download
- ‚úÖ User can choose to download model
- ‚ùå Requires internet for first download

**Implementation:**
```typescript
// Show dialog on first chat attempt:
"Llama model needed (~500MB). Download now? [Download] [Skip]"
// If Skip ‚Üí show message: "Cloud API required, or download model later"
```

---

## üéØ Model Recommendations

| Model | Size | Quality | Speed | RAM Needed |
|-------|------|---------|-------|------------|
| **TinyLlama 1.1B** | 400MB | Good | Fast | 2GB |
| **Llama 3.2 3B** (INT8) | 500MB | Better | Medium | 3GB |
| **Llama 2 7B** (INT8) | 2GB | Best | Slow | 4GB |

**Recommendation: Llama 3.2 3B INT8**
- Best balance of quality/size/speed
- Good for agricultural Q&A
- Works on most mid-range devices (3GB+ RAM)

---

## üìã Implementation Checklist

### Phase 1: Setup (Week 1)
- [ ] Add ONNX Runtime Mobile dependency to `build.gradle`
- [ ] Create `LlamaLocalPlugin.java` structure
- [ ] Set up model conversion pipeline (Llama ‚Üí ONNX ‚Üí INT8)

### Phase 2: Core Integration (Week 2)
- [ ] Implement `LlamaLocalNative.ts` Capacitor bridge
- [ ] Create `LlamaLocal.ts` TypeScript wrapper
- [ ] Build `LLMProvider.ts` unified interface
- [ ] Test model loading and inference

### Phase 3: Integration (Week 3)
- [ ] Update `ChatDrawer.tsx` to use `LLMProvider`
- [ ] Add model download UI/flow
- [ ] Implement automatic fallback logic
- [ ] Test on multiple devices

### Phase 4: Optimization (Week 4)
- [ ] Optimize model quantization (smaller size)
- [ ] Add caching for faster startup
- [ ] Profile and optimize inference speed
- [ ] Test battery impact

---

## üîÑ Migration Path

**Step 1:** Keep Gemini Nano as primary
**Step 2:** Add Llama Local as fallback (same interface)
**Step 3:** Update ChatDrawer to use unified provider
**Step 4:** Test on devices without Gemini Nano support

**No Breaking Changes:**
- Existing Gemini Nano code stays
- New Llama code is additive
- ChatDrawer just calls different provider

---

## ‚ö†Ô∏è Considerations

### Device Compatibility
- **Minimum RAM:** 3GB for Llama 3.2 3B
- **Android Version:** 7+ (API 24+)
- **Storage:** 500MB+ free space for model

### Performance
- **Inference Time:** 2-5 seconds per response (vs 1-2s for Gemini Nano)
- **Battery Impact:** Medium (CPU-intensive, no GPU acceleration)
- **Quality:** Good for text Q&A, not as good as Gemini Nano

### Alternatives if ONNX Too Complex

**Simpler Option:** Use **ML Kit Text Generator** (if Google adds it)
- Or wait for **Gemini API** with local-first mode
- Or use **TensorFlow Lite** (more Android-native)

---

## üìö Resources

1. **ONNX Runtime Mobile:** https://onnxruntime.ai/docs/tutorials/mobile/
2. **Model Conversion:** https://huggingface.co/docs/optimum/onnxruntime/usage_guides/export
3. **Llama Models:** https://huggingface.co/models?search=llama
4. **Quantization Guide:** https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html

---

## ‚úÖ Decision

**Recommended Approach:**
1. ‚úÖ Use **ONNX Runtime Mobile** with **Llama 3.2 3B INT8**
2. ‚úÖ **Download model on first use** (not bundled)
3. ‚úÖ **3-tier fallback:** Gemini Nano ‚Üí Llama ‚Üí Cloud (optional)
4. ‚úÖ Keep existing Gemini Nano code (no breaking changes)

**Estimated Timeline:** 3-4 weeks for full implementation

**Next Step:** Start with ONNX Runtime Mobile setup and model conversion pipeline.
