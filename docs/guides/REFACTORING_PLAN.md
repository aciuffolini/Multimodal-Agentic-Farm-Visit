# ğŸ”„ Complete Refactoring Plan - Farm Visit PWA

## ğŸ¯ Core Vision (Reminder)

**Farm Visit PWA** - Offline-first field data capture with AI assistance:
- âœ… PWA with Capacitor (Android/iOS)
- âœ… Native sensors: GPS, Camera, Microphone, Speaker
- âœ… Online/Offline capable
- âœ… Hybrid LLM: Local edge models (simple tasks) + Cloud (complex reasoning)
- âœ… Future: Ray-Ban Meta Wayfarer Gen 2 integration

---

## ğŸ” Current Problems

### 1. Chat Interface Issues
- âŒ Complex error handling that's not working
- âŒ Too many fallback layers causing confusion
- âŒ Server dependency not clearly separated
- âŒ Error messages not helpful

### 2. Commit Issues
- âŒ Too many uncommitted files
- âŒ Line ending conflicts
- âŒ Large commits mixing unrelated changes

### 3. Architecture Issues
- âŒ LLM provider too complex
- âŒ Web vs Native logic mixed
- âŒ Server dependency unclear
- âŒ Error handling scattered

---

## âœ… Refactoring Strategy

### Phase 1: Simplify Core Architecture (IMMEDIATE)

#### 1.1 Separate Concerns Clearly

```
apps/web/src/
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ LLMProvider.ts          # SIMPLIFIED - Main interface
â”‚   â”‚   â”œâ”€â”€ LocalLLM.ts             # NEW - Handles Nano/Llama (offline)
â”‚   â”‚   â”œâ”€â”€ CloudLLM.ts              # NEW - Handles OpenAI/Anthropic (online)
â”‚   â”‚   â””â”€â”€ LLMStrategy.ts          # NEW - Decides which to use
â”‚   â”œâ”€â”€ sensors/                    # âœ… Already good
â”‚   â”œâ”€â”€ api/                         # SIMPLIFIED - Just API calls
â”‚   â””â”€â”€ storage/                     # NEW - Offline data management
```

#### 1.2 Simplify LLM Provider

**Current Problem**: One giant file with too many responsibilities

**New Structure**:
- `LocalLLM.ts` - Handles offline models (Nano, Llama)
- `CloudLLM.ts` - Handles online models (OpenAI, Anthropic)
- `LLMStrategy.ts` - Decides which to use based on:
  - Online/offline status
  - Model availability
  - Task complexity
- `LLMProvider.ts` - Simple facade that delegates

#### 1.3 Remove Server Dependency for Core Features

**Problem**: Chat requires test server even for offline models

**Solution**: 
- Local models (Nano/Llama) work completely offline
- Cloud models only need server when online
- Clear separation: offline-first, cloud-optional

---

### Phase 2: Refactor Chat Interface (SHORT TERM)

#### 2.1 Simplify Error Handling

**Current**: Complex nested try-catch with multiple error paths

**New**: 
- Single error boundary
- Clear error types
- Simple, actionable messages

#### 2.2 Improve User Experience

- Show model status clearly (online/offline)
- Auto-detect available models
- Clear indicators: "Offline mode" vs "Online mode"
- Better feedback during model switching

---

### Phase 3: Fix Commit Process (IMMEDIATE)

#### 3.1 Clean Up Repository

1. **Organize files**:
   ```
   docs/
     â”œâ”€â”€ guides/          # Installation, build guides
     â”œâ”€â”€ architecture/    # Architecture docs
     â””â”€â”€ troubleshooting/ # Debug guides
   scripts/
     â”œâ”€â”€ start-dev.ps1    # Start dev server
     â”œâ”€â”€ start-api.ps1   # Start API server
     â””â”€â”€ start-both.ps1   # Start both
   ```

2. **Remove temporary files**:
   - Delete all `*_SUMMARY.md`, `*_ANALYSIS.md` debug files
   - Keep only essential documentation

3. **Commit in logical groups**:
   - Core refactoring (one commit)
   - UI improvements (one commit)
   - Documentation (one commit)

---

## ğŸ—ï¸ Proposed New Architecture

### LLM Strategy (Simplified)

```typescript
// LLMStrategy.ts - Simple decision logic
export class LLMStrategy {
  async selectModel(task: Task, context: Context): Promise<LLMProvider> {
    // 1. Check if offline task (can use local)
    if (task.isSimple && await this.hasLocalModel()) {
      return new LocalLLM();
    }
    
    // 2. Check if online and complex task
    if (navigator.onLine && task.isComplex) {
      return new CloudLLM();
    }
    
    // 3. Fallback to local if available
    if (await this.hasLocalModel()) {
      return new LocalLLM();
    }
    
    // 4. Last resort: cloud if online
    if (navigator.onLine) {
      return new CloudLLM();
    }
    
    throw new Error('No LLM available');
  }
}
```

### Local LLM (Offline-First)

```typescript
// LocalLLM.ts - Handles Nano/Llama
export class LocalLLM {
  async stream(input: LLMInput): AsyncGenerator<string> {
    // Try Nano first (Android 14+)
    if (await this.isNanoAvailable()) {
      return this.streamNano(input);
    }
    
    // Try Llama (Android 7+)
    if (await this.isLlamaAvailable()) {
      return this.streamLlama(input);
    }
    
    throw new Error('No local LLM available');
  }
}
```

### Cloud LLM (Online-Only)

```typescript
// CloudLLM.ts - Handles OpenAI/Anthropic
export class CloudLLM {
  async stream(input: LLMInput): AsyncGenerator<string> {
    // Check if server is needed (for local dev)
    if (import.meta.env.DEV) {
      // Use test server in development
      return this.streamViaServer(input);
    } else {
      // Use direct API in production
      return this.streamDirect(input);
    }
  }
}
```

---

## ğŸ“‹ Refactoring Steps

### Step 1: Create New Structure (1-2 hours)

1. Create new files:
   - `apps/web/src/lib/llm/LocalLLM.ts`
   - `apps/web/src/lib/llm/CloudLLM.ts`
   - `apps/web/src/lib/llm/LLMStrategy.ts`

2. Extract logic from `LLMProvider.ts`:
   - Move Nano/Llama code â†’ `LocalLLM.ts`
   - Move OpenAI/Anthropic code â†’ `CloudLLM.ts`
   - Move decision logic â†’ `LLMStrategy.ts`

3. Simplify `LLMProvider.ts` to be a facade

### Step 2: Refactor Chat Interface (1 hour)

1. Simplify `ChatDrawer.tsx`:
   - Remove complex error handling
   - Use simple try-catch
   - Clear error messages

2. Update to use new LLM structure

### Step 3: Clean Repository (30 min)

1. Organize documentation
2. Remove temporary files
3. Create proper folder structure

### Step 4: Commit Everything (15 min)

1. Commit refactoring
2. Commit UI improvements
3. Commit documentation cleanup

---

## ğŸ¯ Success Criteria

After refactoring:

1. âœ… **Chat works offline** (with Nano/Llama)
2. âœ… **Chat works online** (with Cloud API)
3. âœ… **Clear error messages** when models unavailable
4. âœ… **No server required** for offline features
5. âœ… **Clean codebase** - easy to understand
6. âœ… **All changes committed** to GitHub

---

## ğŸš€ Quick Start Refactoring

### Option A: Full Refactor (Recommended)

1. Create new LLM structure
2. Migrate code gradually
3. Test each component
4. Commit when working

### Option B: Incremental Refactor

1. Start with LocalLLM extraction
2. Test offline features
3. Then extract CloudLLM
4. Test online features
5. Finally simplify LLMProvider

---

## ğŸ“ Next Steps

**IMMEDIATE**:
1. âœ… Create this refactoring plan
2. â­ï¸ Start with LocalLLM extraction
3. â­ï¸ Test offline chat
4. â­ï¸ Then CloudLLM extraction
5. â­ï¸ Test online chat
6. â­ï¸ Clean repository
7. â­ï¸ Commit everything

**Would you like me to start the refactoring now?**



